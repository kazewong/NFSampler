{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing sampling result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will show how to analyze the sampling result from `flowMC` using [`arviz`](https://arviz-devs.github.io/arviz/). In particular, we will look at $\\hat{R}$ and effective sample size (ESS) to check convergence and efficiency of the sampling result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the \"dual moon\" distribution from the first tutorial as a testbed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp  # JAX NumPy\n",
    "from jax.scipy.special import logsumexp\n",
    "\n",
    "\n",
    "n_dim = 5\n",
    "\n",
    "\n",
    "def target_dualmoon(x, data):\n",
    "    \"\"\"\n",
    "    Term 2 and 3 separate the distribution and smear it along the first and second dimension\n",
    "    \"\"\"\n",
    "    print(\"compile count\")\n",
    "    term1 = 0.5 * ((jnp.linalg.norm(x - data[\"data\"]) - 2) / 0.1) ** 2\n",
    "    term2 = -0.5 * ((x[:1] + jnp.array([-3.0, 3.0])) / 0.8) ** 2\n",
    "    term3 = -0.5 * ((x[1:2] + jnp.array([-3.0, 3.0])) / 0.6) ** 2\n",
    "    return -(term1 - logsumexp(term2) - logsumexp(term3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run two versions of the sampler to compare diagnostics on the chains: \n",
    "- A full-version of `flowMC` \n",
    "- A version where we deactivate the use of global MCMC steps using the Normalizing Flow. This boils down to a traditional local MCMC, here the Metropolis-Adjusted Lagevin algorithm.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up and running the samplers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flowMC.nfmodel.rqSpline import MaskedCouplingRQSpline\n",
    "from flowMC.sampler.MALA import MALA\n",
    "from flowMC.sampler.Sampler import Sampler\n",
    "\n",
    "\n",
    "# Set up the parameters\n",
    "n_chains = 20\n",
    "\n",
    "n_loop_training = n_loop_production = 20\n",
    "n_local_steps = 100\n",
    "n_global_steps = 10\n",
    "num_epochs = 5\n",
    "\n",
    "learning_rate = 0.005\n",
    "momentum = 0.9\n",
    "batch_size = 5000\n",
    "max_samples = 5000\n",
    "\n",
    "step_size = 1e-1  # step size for MALA\n",
    "\n",
    "print(\"Preparing RNG keys\")\n",
    "\n",
    "rng_key = jax.random.PRNGKey(42)\n",
    "rng_key, subkey = jax.random.split(rng_key)\n",
    "\n",
    "print(\"Initializing chains, normalizing flow model and local MCMC sampler\")\n",
    "\n",
    "initial_position = jax.random.normal(subkey, shape=(n_chains, n_dim)) * 1\n",
    "rng_key, subkey = jax.random.split(rng_key)\n",
    "model = MaskedCouplingRQSpline(n_dim, 4, [32, 32], 8, subkey)\n",
    "MALA_Sampler = MALA(target_dualmoon, True, {\"step_size\": step_size})\n",
    "\n",
    "print(\"Initializing samplers classes\")\n",
    "\n",
    "nf_sampler = Sampler(\n",
    "    n_dim,\n",
    "    rng_key,\n",
    "    {\"data\": jnp.zeros(5)},\n",
    "    MALA_Sampler,\n",
    "    model,\n",
    "    n_loop_training=n_loop_training,\n",
    "    n_loop_production=n_loop_production,\n",
    "    n_local_steps=n_local_steps,\n",
    "    n_global_steps=n_global_steps,\n",
    "    n_chains=n_chains,\n",
    "    n_epochs=num_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    momentum=momentum,\n",
    "    batch_size=batch_size,\n",
    "    use_global=True,\n",
    ")\n",
    "\n",
    "rng_key, subkey = jax.random.split(rng_key)\n",
    "local_sampler = Sampler(\n",
    "    n_dim,\n",
    "    subkey,\n",
    "    {\"data\": jnp.zeros(5)},\n",
    "    MALA_Sampler,\n",
    "    model,\n",
    "    n_loop_training=0,\n",
    "    n_loop_production=1,\n",
    "    n_local_steps=n_local_steps * (n_loop_production + n_loop_training),\n",
    "    n_global_steps=n_global_steps,\n",
    "    n_chains=n_chains,\n",
    "    n_epochs=num_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    momentum=momentum,\n",
    "    batch_size=batch_size,\n",
    "    use_global=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running local sampling\")\n",
    "\n",
    "local_sampler.sample(initial_position, {\"data\": jnp.zeros(n_dim)})\n",
    "summary_local = local_sampler.get_sampler_state()\n",
    "(\n",
    "    chains_local,\n",
    "    log_prob_local,\n",
    "    local_accs_local,\n",
    "    global_accs_local,\n",
    ") = summary_local.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running flowMC sampling\")\n",
    "nf_sampler.sample(initial_position, {\"data\": jnp.zeros(n_dim)})\n",
    "summary = nf_sampler.get_sampler_state()\n",
    "chains, log_prob, local_accs, global_accs = summary.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyze the output of the chains we will use the widely package `arviz`. Arviz uses a specific type of data structure, `InferenceData` which we can easily convert to from numpy arrays. Since we only kept the production samples from the `flowMC` sampler, we also only consider get rid of the draws of the MALA samples in the bur-in phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data = az.convert_to_inference_data(np.array(chains))\n",
    "data_local = az.convert_to_inference_data(np.array(chains_local))\n",
    "\n",
    "n_draws_local = data_local.posterior.dims[\"draw\"]\n",
    "\n",
    "print(\"Number of total local samples: \", n_draws_local)\n",
    "\n",
    "data_local_ = data_local.sel(draw=slice(1800, None))\n",
    "n_draws_local_ = data_local_.posterior.dims[\"draw\"]\n",
    "n_draws = data.posterior.dims[\"draw\"]\n",
    "print(\n",
    "    \"Number of samples: \",\n",
    "    n_draws,\n",
    "    \"- Number of samples local: \",\n",
    "    n_draws_local_,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we can use the diagnostic tools implemented in arviz. To start with we can visualize the samples obtained by drawing the kernel density estimations in pair plots. At first sight, the local and the `flowMC` sampler look of comparable quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = az.plot_pair(data, kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = az.plot_pair(data_local_, kind=\"kde\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if we consider the next sanity check we will see that the local sampler actually does not mix. This next check is to compute the [rhat](https://arviz-devs.github.io/arviz/api/generated/arviz.rhat.html?highlight=rhat): $$\\hat{R} = \\frac{\\text{inter-chain variance}}{\\text{within-chain variance}}$$\n",
    "\n",
    "When chains are properly mixing, a single chain has comparative variance to the pool of states visited by the batch of chains and $\\hat{R} \\to 1$. Conversely, when the chains are not mixing $\\hat{R} > 1$. The results below show that MALA is not mixing along the dimensions where the marginal of the distribution are multimodal: `x_0`and `x_1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhat_flownmc = az.rhat(data)\n",
    "rhat_local = az.rhat(data_local_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "width = 0.35\n",
    "x = data.posterior.x_dim_0.data\n",
    "\n",
    "plt.bar(\n",
    "    x - width / 2, rhat_flownmc.to_array().data.squeeze(), width, label=\"flowMC\"\n",
    ")\n",
    "plt.bar(\n",
    "    x + width / 2, rhat_local.to_array().data.squeeze(), width, label=\"MALA\"\n",
    ")\n",
    "plt.axhline(1.0, color=\"k\", linestyle=\"--\")\n",
    "plt.xticks(x, [\"x{:d}\".format(i) for i in x])\n",
    "plt.ylabel(\"Rhat\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another quantity of interest to compare the performance of sampling algorithms is to look at the effective sample size (ESS). This quantity is computed from an estimate of the autocorrelation of the chains and represents the number of samples that could be considered as independent within a given sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_draws = data.posterior.draw.shape[0]\n",
    "\n",
    "ess_flownmc = az.ess(data)\n",
    "ess_local = az.ess(data_local_)\n",
    "\n",
    "width = 0.35\n",
    "x = data.posterior.x_dim_0.data\n",
    "\n",
    "plt.bar(\n",
    "    x - width / 2,\n",
    "    ess_flownmc.to_array().data.squeeze() / n_draws,\n",
    "    width,\n",
    "    label=\"flowMC\",\n",
    ")\n",
    "plt.bar(\n",
    "    x + width / 2,\n",
    "    ess_local.to_array().data.squeeze() / n_draws,\n",
    "    width,\n",
    "    label=\"MALA\",\n",
    ")\n",
    "plt.ylabel(\"ESS per draw\")\n",
    "plt.xticks(x, [\"x{:d}\".format(i) for i in x])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The global moves proposed by the normalizing flow of `flowMC` are completely independent from the previous state of the chain, hence when the global acceptance is none zero the decorrelation is very fast. Here we can see that it oscillates around $35\\%$, hence the much better ESS for flowMC compared to MALA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Global Acceptance\")\n",
    "plt.plot(global_accs.mean(0))\n",
    "plt.xlabel(\"iteration\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('flowMC')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b400b6332f08c858a49f324343ace43c214462970fdeaeb2e184c655690bde9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
